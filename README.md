# P04-Weather-Risk
![Header IMG](Images/DALL·E_Weather_Risk.jpg)

## Thesis
This project aims to leverage neural networks and weather data to predict housing risks, enhancing the accuracy of risk assessments amidst escalating extreme weather events. By analyzing historical weather patterns and housing data, including damages and construction details, we'll train a model to forecast weather-related housing vulnerabilities. This research seeks to provide actionable insights for homeowners, insurers, and policymakers, promoting more resilient housing strategies in the face of climate change.

## File Location
- In 'Images' you can find all images used in this project
- In 'Data_cleaning' you can find a few python notebooks used to clean up our initial DF
- In 'Machine_Learning' you can find a all of our machine learning work.
- In 'Resources_Output' you can find a few files for our cleaned data some of this is too large and is gitignored
- At https://public.tableau.com/app/profile/riley.capps/viz/USWeatherVSHousingPrices/Texas you can find a basic tableau project for this
- At 'https://docs.google.com/presentation/d/1VCf4qmWdVo3sgTju3IOdZOJrTCPjoLN1Isv1cCpPP9Y/edit#slide=id.g26c8fcd9e6a_0_95' you can find our presentation used for this project


# Sources
## Weather
- https://www.ncei.noaa.gov/pub/data/swdi/stormevents/
- https://drive.google.com/file/d/1bKw42VGYukQC664Zw9bPtlLiJvI7nj09/view?usp=sharing
- https://www.ncei.noaa.gov/access/monitoring/reference-maps/

## Housing
- https://www.zillow.com/research/data/	

## Model Testing
* Started trying to use guide that created a ‘window’ class in the tf ‘timeseries’ tutorial  
	* Unsuccessful/unable to understand how to tweak the model  
* Feature engineering  
	* Dropped weather events, and regions very early in the process because we were having trouble achieving a r^2 value greater than 0  
	* Realized extremely late in the process that we were inefficiently testing how useful the features  
	* Running a PCA analysis earlier in the process could potentially led to us keeping in PCA  
	* Ran PCA analysis late in the process to evaluate the explained variance of the datetime parameter engineered in different ways:  
	* Attempted:  
	    ** Date split up into days, months, years, day of year, week of year, each as a different feature.  
	    ** Also tried grouping the datetimes by days, months, and years  
        ** Years created too small of a dataset(upon reflection, likely could have run a linear regression on the years)
	        *** Days created a dataset where the explained variance was almost equal to property damage implying there was little relationship
	    	*** Turned the date into a unix timestamp  
	        *** Attempted grouping the dates by days, months and years  
	        *** Attempted taking the sine and cosine of the timestamp  
	        *** Attempted taking the sine and cosine of the timestamp split into days, months and years  
	    ** Learning rate  
	    ** Reducing the learning rate reduced the amount of overfitting.  
	    ** Used a higher learning rate when attempting to test many hyperparameters with the parameter grid for loop to quickly assess the weakest hyperparameters.  
	* Timesteps  
	* Timestep with the greatest r^2 value across the models trained was 7   
	* Batch size  
	    ** Reduced r^2 values more than testing the entire set at once  
	* Layers  
    	* Neurons  
	* Return sequences  
        	** Created a return sequence that tests the r^2 value for validation data and autostops the model from training


## Citations
Header Image generated by ChatGPT using DALL-E

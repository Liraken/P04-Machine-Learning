{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from pprint import pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>...</th>\n",
       "      <th>END_RANGE</th>\n",
       "      <th>END_AZIMUTH</th>\n",
       "      <th>END_LOCATION</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201402</td>\n",
       "      <td>18</td>\n",
       "      <td>1000</td>\n",
       "      <td>201402</td>\n",
       "      <td>18</td>\n",
       "      <td>2000</td>\n",
       "      <td>83473.0</td>\n",
       "      <td>503953</td>\n",
       "      <td>NEW HAMPSHIRE</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low pressure developing south of Long Island a...</td>\n",
       "      <td>Eight to twelve inches of snow fell across eas...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201402</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>201402</td>\n",
       "      <td>5</td>\n",
       "      <td>2300</td>\n",
       "      <td>83491.0</td>\n",
       "      <td>504065</td>\n",
       "      <td>NEW HAMPSHIRE</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low pressure moving off the mid-Atlantic coast...</td>\n",
       "      <td>Six to twelve inches of snow fell across easte...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201401</td>\n",
       "      <td>18</td>\n",
       "      <td>1000</td>\n",
       "      <td>201401</td>\n",
       "      <td>19</td>\n",
       "      <td>700</td>\n",
       "      <td>82185.0</td>\n",
       "      <td>494521</td>\n",
       "      <td>NEW HAMPSHIRE</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low pressure brought a brief period of heavy s...</td>\n",
       "      <td>Four to eight inches of snow fell across easte...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201411</td>\n",
       "      <td>26</td>\n",
       "      <td>1000</td>\n",
       "      <td>201411</td>\n",
       "      <td>27</td>\n",
       "      <td>1000</td>\n",
       "      <td>91728.0</td>\n",
       "      <td>549746</td>\n",
       "      <td>NEW HAMPSHIRE</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A strong coastal storm moved up the east coast...</td>\n",
       "      <td>Six to eight inches of snow fell across easter...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201402</td>\n",
       "      <td>13</td>\n",
       "      <td>630</td>\n",
       "      <td>201402</td>\n",
       "      <td>14</td>\n",
       "      <td>800</td>\n",
       "      <td>83476.0</td>\n",
       "      <td>503982</td>\n",
       "      <td>NEW HAMPSHIRE</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A significant winter storm brought six to twel...</td>\n",
       "      <td>Five to eight inches of snow fell across easte...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
       "0           201402         18        1000         201402       18      2000   \n",
       "1           201402          5         300         201402        5      2300   \n",
       "2           201401         18        1000         201401       19       700   \n",
       "3           201411         26        1000         201411       27      1000   \n",
       "4           201402         13         630         201402       14       800   \n",
       "\n",
       "   EPISODE_ID  EVENT_ID          STATE  STATE_FIPS  ...  END_RANGE  \\\n",
       "0     83473.0    503953  NEW HAMPSHIRE        33.0  ...        NaN   \n",
       "1     83491.0    504065  NEW HAMPSHIRE        33.0  ...        NaN   \n",
       "2     82185.0    494521  NEW HAMPSHIRE        33.0  ...        NaN   \n",
       "3     91728.0    549746  NEW HAMPSHIRE        33.0  ...        NaN   \n",
       "4     83476.0    503982  NEW HAMPSHIRE        33.0  ...        NaN   \n",
       "\n",
       "  END_AZIMUTH END_LOCATION BEGIN_LAT  BEGIN_LON END_LAT END_LON  \\\n",
       "0         NaN          NaN       NaN        NaN     NaN     NaN   \n",
       "1         NaN          NaN       NaN        NaN     NaN     NaN   \n",
       "2         NaN          NaN       NaN        NaN     NaN     NaN   \n",
       "3         NaN          NaN       NaN        NaN     NaN     NaN   \n",
       "4         NaN          NaN       NaN        NaN     NaN     NaN   \n",
       "\n",
       "                                   EPISODE_NARRATIVE  \\\n",
       "0  Low pressure developing south of Long Island a...   \n",
       "1  Low pressure moving off the mid-Atlantic coast...   \n",
       "2  Low pressure brought a brief period of heavy s...   \n",
       "3  A strong coastal storm moved up the east coast...   \n",
       "4  A significant winter storm brought six to twel...   \n",
       "\n",
       "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
       "0  Eight to twelve inches of snow fell across eas...         CSV  \n",
       "1  Six to twelve inches of snow fell across easte...         CSV  \n",
       "2  Four to eight inches of snow fell across easte...         CSV  \n",
       "3  Six to eight inches of snow fell across easter...         CSV  \n",
       "4  Five to eight inches of snow fell across easte...         CSV  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use folder path as the path to folder with all the yearly storm data\n",
    "folder_path='.'\n",
    "dfs=[]\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Read CSV file into a dataframe\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        # Append dataframe to the list\n",
    "        dfs.append(df)\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1671202"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the columns we obviously do not need\n",
    "combined_df=combined_df[['BEGIN_DATE_TIME','DAMAGE_PROPERTY','STATE','EVENT_TYPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with null values in the 3 columns\n",
    "df=combined_df.dropna(subset=['DAMAGE_PROPERTY','BEGIN_LAT','END_LAT'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert the values in 'DAMAGE_PROPERTY' to float\n",
    "def value_to_float(x):\n",
    "    if type(x) == float or type(x) == int:\n",
    "        return x\n",
    "    if 'K' in x:\n",
    "        if len(x) > 1:\n",
    "            return float(x.replace('K', '')) * 1000\n",
    "        return 1000.0\n",
    "    if 'M' in x:\n",
    "        if len(x) > 1:\n",
    "            return float(x.replace('M', '')) * 1000000\n",
    "        return 1000000.0\n",
    "    if 'B' in x:\n",
    "        return float(x.replace('B', '')) * 1000000000\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DAMAGE_PROPERTY']=df['DAMAGE_PROPERTY'].apply(value_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert values in 'BEGIN_DATE_TIME' AND 'END_DATE_TIME' to datetime variables\n",
    "def convert_to_datetime(date_string):\n",
    "    format_string = '%d-%b-%y %H:%M:%S'\n",
    "    return dt.strptime(date_string, format_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BEGIN_DATE_TIME']=df['BEGIN_DATE_TIME'].apply(convert_to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the null values in 'DAMAGE_PROPERTY' column with 0s\n",
    "df['DAMAGE_PROPERTY']=df['DAMAGE_PROPERTY'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Filtering/Cleanup\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1671202"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['STATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE'] = df['STATE'].str.upper()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding climate region data, dropping state data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of US states for filtering\n",
    "# hawaii and alaska dropped\n",
    "# we only want the lower 48 mainland US states for data consistency when training\n",
    "us_states = [ \"ALABAMA\", \"ARIZONA\", \"ARKANSAS\", \"CALIFORNIA\", \"COLORADO\", \"CONNECTICUT\", \"DELAWARE\", \"FLORIDA\", \"GEORGIA\", \n",
    "             \"IDAHO\", \"ILLINOIS\", \"INDIANA\", \"IOWA\", \"KANSAS\", \"KENTUCKY\", \"LOUISIANA\", \"MAINE\", \"MARYLAND\", \"MASSACHUSETTS\", \n",
    "             \"MICHIGAN\", \"MINNESOTA\", \"MISSISSIPPI\", \"MISSOURI\", \"MONTANA\", \"NEBRASKA\", \"NEVADA\", \"NEW HAMPSHIRE\", \"NEW JERSEY\", \n",
    "             \"NEW MEXICO\", \"NEW YORK\", \"NORTH CAROLINA\", \"NORTH DAKOTA\", \"OHIO\", \"OKLAHOMA\", \"OREGON\", \"PENNSYLVANIA\", \"RHODE ISLAND\", \n",
    "             \"SOUTH CAROLINA\", \"SOUTH DAKOTA\", \"TENNESSEE\", \"TEXAS\", \"UTAH\", \"VERMONT\", \"VIRGINIA\", \"WASHINGTON\", \"WEST VIRGINIA\", \"WISCONSIN\", \"WYOMING\"]\n",
    "\n",
    "# dictionary of NCEI NOAA climate regions to add to our data\n",
    "climate_region_dict = {\n",
    "    \"NORTHEAST\":    [\"CONNECTICUT\", \"DELAWARE\", \"MAINE\", \"MARYLAND\", \"MASSACHUSETTS\", \"NEW HAMPSHIRE\", \"NEW JERSEY\", \"NEW YORK\", \"PENNSYLVANIA\", \"RHODE ISLAND\", \"VERMONT\"],\n",
    "    \"UPPER MIDWEST\":[\"IOWA\", \"MICHIGAN\", \"MINNESOTA\", \"WISCONSIN\"],\n",
    "    \"OHIO VALLEY\":  [\"ILLINOIS\", \"INDIANA\", \"KENTUCKY\", \"MISSOURI\", \"OHIO\", \"TENNESSEE\", \"WEST VIRGINIA\"],\n",
    "    \"SOUTHEAST\":    [\"ALABAMA\", \"FLORIDA\", \"GEORGIA\", \"NORTH CAROLINA\", \"SOUTH CAROLINA\", \"VIRGINIA\"],\n",
    "    \"NORTHERN ROCKIES AND PLAINS\": [\"MONTANA\", \"NEBRASKA\", \"NORTH DAKOTA\", \"SOUTH DAKOTA\", \"WYOMING\"],\n",
    "    \"SOUTH\":        [\"ARKANSAS\", \"KANSAS\", \"LOUISIANA\", \"MISSISSIPPI\", \"OKLAHOMA\", \"TEXAS\"],\n",
    "    \"SOUTHWEST\":    [\"ARIZONA\", \"COLORADO\", \"NEW MEXICO\", \"UTAH\"],\n",
    "    \"NORTHWEST\":    [\"IDAHO\", \"OREGON\", \"WASHINGTON\"],\n",
    "    \"WEST\":         [\"CALIFORNIA\", \"NEVADA\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping non-states\n",
    "df = df[df['STATE'].isin(us_states)]\n",
    "\n",
    "df['STATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_region_dict = {}\n",
    "for key in climate_region_dict:\n",
    "    for item in climate_region_dict[key]:\n",
    "        formatted_region_dict[item] = key\n",
    "\n",
    "formatted_region_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = region classification\n",
    "# value = list of states\n",
    "# this is probably not a very efficient way to do this\n",
    "df['REGION'] = df['STATE'].map(formatted_region_dict)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Type Double Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EVENT_TYPE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby([\"EVENT_TYPE\"])\n",
    "grouped.sum([\"DAMAGE_PROPERTY\"]).drop(columns=[\"BEGIN_LAT\", \"BEGIN_LON\", \"END_LAT\", \"END_LON\"]).sort_values(\"DAMAGE_PROPERTY\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EVENT_TYPE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_trans_dict = {\n",
    "    'THUNDERSTORM WINDS/FLOODING': 'Flood',\n",
    "    'HAIL/ICY ROADS': 'Hail',\n",
    "    'HAIL FLOODING': 'Flood',\n",
    "    'THUNDERSTORM WINDS/FLASH FLOOD': 'Flash Flood',\n",
    "    'THUNDERSTORM WINDS LIGHTNING': 'Lightning',\n",
    "    'THUNDERSTORM WIND/ TREES': 'Thunderstorm Wind',\n",
    "    'THUNDERSTORM WIND/ TREE': 'Thunderstorm Wind',\n",
    "    'THUNDERSTORM WINDS FUNNEL CLOU': 'Funnel Cloud',\n",
    "    'TORNADO/WATERSPOUT': 'Waterspout',\n",
    "    'THUNDERSTORM WINDS/HEAVY RAIN': 'Heavy Rain',\n",
    "    'THUNDERSTORM WINDS HEAVY RAIN': 'Heavy Rain',\n",
    "    'THUNDERSTORM WINDS/ FLOOD': 'Flood',\n",
    "}\n",
    "\n",
    "\n",
    "rest_of_cats = ['Tornado', 'Thunderstorm Wind', 'Hail', 'Winter Storm', 'Cold/Wind Chill',\n",
    "       'Heavy Snow', 'Flood', 'High Wind', 'Flash Flood', 'Blizzard',\n",
    "       'Ice Storm', 'Lightning', 'Frost/Freeze', 'Heavy Rain',\n",
    "       'Strong Wind', 'Coastal Flood', 'Wildfire', 'Funnel Cloud',\n",
    "       'Winter Weather', 'Waterspout', 'Drought', 'Debris Flow', 'Heat',\n",
    "       'High Surf', 'Tropical Storm', 'Dust Devil', 'Dense Fog',\n",
    "       'Hurricane (Typhoon)', 'Marine High Wind', 'Dust Storm',\n",
    "       'Storm Surge/Tide', 'Lake-Effect Snow', 'Rip Current', 'Avalanche',\n",
    "       'Seiche', 'Extreme Cold/Wind Chill', 'Excessive Heat', 'Tsunami',\n",
    "       'Sleet', 'Freezing Fog', 'Lakeshore Flood',\n",
    "       'Astronomical Low Tide', 'Tropical Depression', 'Dense Smoke',\n",
    "       'Sneakerwave', 'Hurricane']\n",
    "\n",
    "all_event_transforms = {item: item for item in rest_of_cats}\n",
    "all_event_transforms.update(category_trans_dict)\n",
    "\n",
    "pp(all_event_transforms.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EVENT_TYPE\"] = df[\"EVENT_TYPE\"].map(all_event_transforms)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify remaining categories\n",
    "df[\"EVENT_TYPE\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Thoughts on Event Type Consolidation\n",
    "***\n",
    "\n",
    "We may want to consider more merges including:\n",
    "\n",
    "Hurricane (Typhoon) -> 'Hurricane'\n",
    "\n",
    "All Floods (except maybe flash floods) -> Flood\n",
    "\n",
    "(storm floods already filtered are a bit ambiguous in this case)\n",
    "\n",
    "**Ideally, we either want to drop unneeded categories or run them through an unsupervised clustering algorithm to bring the total category count down to <10 for easier neural network processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding, final preprocessing DF\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check the columns to see what we need encoded\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns to encode: \"REGION\", \"EVENT_TYPE\"\n",
    "\n",
    "Could also be worthwhile to target data with only lat-long info and run clustering there, but that is its own endeavor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cols_df = pd.get_dummies(df[[\"REGION\", \"EVENT_TYPE\"]]).astype(int)\n",
    "\n",
    "encoded_cols_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_df = pd.merge(df, encoded_cols_df, on=df.index).drop(columns=[\"REGION\", \"EVENT_TYPE\", \"key_0\"])\n",
    "\n",
    "preproc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path=#Replace with intended output path\n",
    "preproc_df.to_csv(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

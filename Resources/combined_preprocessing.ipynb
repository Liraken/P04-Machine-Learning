{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from pprint import pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use folder path as the path to folder with all the yearly storm data\n",
    "folder_path='P04-Weather-Risk/Resources/'\n",
    "dfs=[]\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Read CSV file into a dataframe\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        # Append dataframe to the list\n",
    "        dfs.append(df)\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the columns we obviously do not need\n",
    "combined_df=combined_df[['BEGIN_DATE_TIME','DAMAGE_PROPERTY','STATE','EVENT_TYPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with null values in the 3 columns\n",
    "df=combined_df.dropna(subset=['DAMAGE_PROPERTY','BEGIN_LAT','END_LAT'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert the values in 'DAMAGE_PROPERTY' to float\n",
    "def value_to_float(x):\n",
    "    if type(x) == float or type(x) == int:\n",
    "        return x\n",
    "    if 'K' in x:\n",
    "        if len(x) > 1:\n",
    "            return float(x.replace('K', '')) * 1000\n",
    "        return 1000.0\n",
    "    if 'M' in x:\n",
    "        if len(x) > 1:\n",
    "            return float(x.replace('M', '')) * 1000000\n",
    "        return 1000000.0\n",
    "    if 'B' in x:\n",
    "        return float(x.replace('B', '')) * 1000000000\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DAMAGE_PROPERTY']=df['DAMAGE_PROPERTY'].apply(value_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert values in 'BEGIN_DATE_TIME' AND 'END_DATE_TIME' to datetime variables\n",
    "def convert_to_datetime(date_string):\n",
    "    format_string = '%d-%b-%y %H:%M:%S'\n",
    "    return dt.strptime(date_string, format_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BEGIN_DATE_TIME']=df['BEGIN_DATE_TIME'].apply(convert_to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the null values in 'DAMAGE_PROPERTY' column with 0s\n",
    "df['DAMAGE_PROPERTY']=df['DAMAGE_PROPERTY'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Filtering/Cleanup\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1671202"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['STATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE'] = df['STATE'].str.upper()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding climate region data, dropping state data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of US states for filtering\n",
    "# hawaii and alaska dropped\n",
    "# we only want the lower 48 mainland US states for data consistency when training\n",
    "us_states = [ \"ALABAMA\", \"ARIZONA\", \"ARKANSAS\", \"CALIFORNIA\", \"COLORADO\", \"CONNECTICUT\", \"DELAWARE\", \"FLORIDA\", \"GEORGIA\", \n",
    "             \"IDAHO\", \"ILLINOIS\", \"INDIANA\", \"IOWA\", \"KANSAS\", \"KENTUCKY\", \"LOUISIANA\", \"MAINE\", \"MARYLAND\", \"MASSACHUSETTS\", \n",
    "             \"MICHIGAN\", \"MINNESOTA\", \"MISSISSIPPI\", \"MISSOURI\", \"MONTANA\", \"NEBRASKA\", \"NEVADA\", \"NEW HAMPSHIRE\", \"NEW JERSEY\", \n",
    "             \"NEW MEXICO\", \"NEW YORK\", \"NORTH CAROLINA\", \"NORTH DAKOTA\", \"OHIO\", \"OKLAHOMA\", \"OREGON\", \"PENNSYLVANIA\", \"RHODE ISLAND\", \n",
    "             \"SOUTH CAROLINA\", \"SOUTH DAKOTA\", \"TENNESSEE\", \"TEXAS\", \"UTAH\", \"VERMONT\", \"VIRGINIA\", \"WASHINGTON\", \"WEST VIRGINIA\", \"WISCONSIN\", \"WYOMING\"]\n",
    "\n",
    "# dictionary of NCEI NOAA climate regions to add to our data\n",
    "climate_region_dict = {\n",
    "    \"NORTHEAST\":    [\"CONNECTICUT\", \"DELAWARE\", \"MAINE\", \"MARYLAND\", \"MASSACHUSETTS\", \"NEW HAMPSHIRE\", \"NEW JERSEY\", \"NEW YORK\", \"PENNSYLVANIA\", \"RHODE ISLAND\", \"VERMONT\"],\n",
    "    \"UPPER MIDWEST\":[\"IOWA\", \"MICHIGAN\", \"MINNESOTA\", \"WISCONSIN\"],\n",
    "    \"OHIO VALLEY\":  [\"ILLINOIS\", \"INDIANA\", \"KENTUCKY\", \"MISSOURI\", \"OHIO\", \"TENNESSEE\", \"WEST VIRGINIA\"],\n",
    "    \"SOUTHEAST\":    [\"ALABAMA\", \"FLORIDA\", \"GEORGIA\", \"NORTH CAROLINA\", \"SOUTH CAROLINA\", \"VIRGINIA\"],\n",
    "    \"NORTHERN ROCKIES AND PLAINS\": [\"MONTANA\", \"NEBRASKA\", \"NORTH DAKOTA\", \"SOUTH DAKOTA\", \"WYOMING\"],\n",
    "    \"SOUTH\":        [\"ARKANSAS\", \"KANSAS\", \"LOUISIANA\", \"MISSISSIPPI\", \"OKLAHOMA\", \"TEXAS\"],\n",
    "    \"SOUTHWEST\":    [\"ARIZONA\", \"COLORADO\", \"NEW MEXICO\", \"UTAH\"],\n",
    "    \"NORTHWEST\":    [\"IDAHO\", \"OREGON\", \"WASHINGTON\"],\n",
    "    \"WEST\":         [\"CALIFORNIA\", \"NEVADA\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping non-states\n",
    "df = df[df['STATE'].isin(us_states)]\n",
    "\n",
    "df['STATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_region_dict = {}\n",
    "for key in climate_region_dict:\n",
    "    for item in climate_region_dict[key]:\n",
    "        formatted_region_dict[item] = key\n",
    "\n",
    "formatted_region_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = region classification\n",
    "# value = list of states\n",
    "# this is probably not a very efficient way to do this\n",
    "df['REGION'] = df['STATE'].map(formatted_region_dict)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Type Double Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EVENT_TYPE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby([\"EVENT_TYPE\"])\n",
    "grouped.sum([\"DAMAGE_PROPERTY\"]).drop(columns=[\"BEGIN_LAT\", \"BEGIN_LON\", \"END_LAT\", \"END_LON\"]).sort_values(\"DAMAGE_PROPERTY\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EVENT_TYPE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_trans_dict = {\n",
    "    'THUNDERSTORM WINDS/FLOODING': 'Flood',\n",
    "    'HAIL/ICY ROADS': 'Hail',\n",
    "    'HAIL FLOODING': 'Flood',\n",
    "    'THUNDERSTORM WINDS/FLASH FLOOD': 'Flash Flood',\n",
    "    'THUNDERSTORM WINDS LIGHTNING': 'Lightning',\n",
    "    'THUNDERSTORM WIND/ TREES': 'Thunderstorm Wind',\n",
    "    'THUNDERSTORM WIND/ TREE': 'Thunderstorm Wind',\n",
    "    'THUNDERSTORM WINDS FUNNEL CLOU': 'Funnel Cloud',\n",
    "    'TORNADO/WATERSPOUT': 'Waterspout',\n",
    "    'THUNDERSTORM WINDS/HEAVY RAIN': 'Heavy Rain',\n",
    "    'THUNDERSTORM WINDS HEAVY RAIN': 'Heavy Rain',\n",
    "    'THUNDERSTORM WINDS/ FLOOD': 'Flood',\n",
    "}\n",
    "\n",
    "\n",
    "rest_of_cats = ['Tornado', 'Thunderstorm Wind', 'Hail', 'Winter Storm', 'Cold/Wind Chill',\n",
    "       'Heavy Snow', 'Flood', 'High Wind', 'Flash Flood', 'Blizzard',\n",
    "       'Ice Storm', 'Lightning', 'Frost/Freeze', 'Heavy Rain',\n",
    "       'Strong Wind', 'Coastal Flood', 'Wildfire', 'Funnel Cloud',\n",
    "       'Winter Weather', 'Waterspout', 'Drought', 'Debris Flow', 'Heat',\n",
    "       'High Surf', 'Tropical Storm', 'Dust Devil', 'Dense Fog',\n",
    "       'Hurricane (Typhoon)', 'Marine High Wind', 'Dust Storm',\n",
    "       'Storm Surge/Tide', 'Lake-Effect Snow', 'Rip Current', 'Avalanche',\n",
    "       'Seiche', 'Extreme Cold/Wind Chill', 'Excessive Heat', 'Tsunami',\n",
    "       'Sleet', 'Freezing Fog', 'Lakeshore Flood',\n",
    "       'Astronomical Low Tide', 'Tropical Depression', 'Dense Smoke',\n",
    "       'Sneakerwave', 'Hurricane']\n",
    "\n",
    "all_event_transforms = {item: item for item in rest_of_cats}\n",
    "all_event_transforms.update(category_trans_dict)\n",
    "\n",
    "pp(all_event_transforms.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EVENT_TYPE\"] = df[\"EVENT_TYPE\"].map(all_event_transforms)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify remaining categories\n",
    "df[\"EVENT_TYPE\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Thoughts on Event Type Consolidation\n",
    "***\n",
    "\n",
    "We may want to consider more merges including:\n",
    "\n",
    "Hurricane (Typhoon) -> 'Hurricane'\n",
    "\n",
    "All Floods (except maybe flash floods) -> Flood\n",
    "\n",
    "(storm floods already filtered are a bit ambiguous in this case)\n",
    "\n",
    "**Ideally, we either want to drop unneeded categories or run them through an unsupervised clustering algorithm to bring the total category count down to <10 for easier neural network processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding, final preprocessing DF\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check the columns to see what we need encoded\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns to encode: \"REGION\", \"EVENT_TYPE\"\n",
    "\n",
    "Could also be worthwhile to target data with only lat-long info and run clustering there, but that is its own endeavor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cols_df = pd.get_dummies(df[[\"REGION\", \"EVENT_TYPE\"]]).astype(int)\n",
    "\n",
    "encoded_cols_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_df = pd.merge(df, encoded_cols_df, on=df.index).drop(columns=[\"REGION\", \"EVENT_TYPE\", \"key_0\"])\n",
    "\n",
    "preproc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path=#Replace with intended output path\n",
    "preproc_df.to_csv(output_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
